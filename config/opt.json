{
    "accum_count": 1, 
    "adagrad_accumulator_init": 0, 
    "adam_beta1": 0.9, 
    "adam_beta2": 0.999, 
    "batch_size": 64, 
    "batch_type": "sents", 
    "bridge": false, 
    "brnn": false, 
    "cnn_kernel_width": 3, 
    "context_gate": null, 
    "copy_attn": false, 
    "copy_attn_force": false, 
    "copy_loss_by_seqlength": false, 
    "coverage_attn": false, 
    "data": "data/demo",
    "dec_layers": 2, 
    "decay_method": "", 
    "decay_steps": 10000, 
    "decoder_type": "rnn", 
    "device_id": 0, 
    "dropout": 0.3, 
    "enc_layers": 2, 
    "encoder_type": "rnn", 
    "epochs": 0, 
    "exp": "", 
    "exp_host": "", 
    "feat_merge": "concat", 
    "feat_vec_exponent": 0.7, 
    "feat_vec_size": -1, 
    "fix_word_vecs_dec": false, 
    "fix_word_vecs_enc": false, 
    "global_attention": "general", 
    "gpu_backend": "nccl", 
    "gpu_rank": 0, 
    "gpu_verbose_level": 0, 
    "gpuid": [
        0
    ], 
    "heads": 8, 
    "input_feed": 1, 
    "keep_checkpoint": -1, 
    "label_smoothing": 0.0, 
    "lambda_coverage": 1, 
    "layers": -1, 
    "learning_rate": 1.0, 
    "learning_rate_decay": 0.5, 
    "log_file": "", 
    "max_generator_batches": 32, 
    "max_grad_norm": 5, 
    "model_type": "text", 
    "normalization": "sents", 
    "optim": "sgd", 
    "param_init": 0.1, 
    "param_init_glorot": false, 
    "position_encoding": false, 
    "pre_word_vecs_dec": null, 
    "pre_word_vecs_enc": null, 
    "report_every": 50, 
    "reuse_copy_attn": false, 
    "rnn_size": 500, 
    "rnn_type": "LSTM", 
    "sample_rate": 16000, 
    "save_checkpoint_steps": 5000, 
    "save_model": "temp/model",
    "seed": -1, 
    "self_attn_type": "scaled-dot", 
    "share_decoder_embeddings": false, 
    "share_embeddings": false, 
    "src_word_vec_size": 500, 
    "start_decay_steps": 50000, 
    "tensorboard": false, 
    "tensorboard_log_dir": "runs/onmt", 
    "tgt_word_vec_size": 500, 
    "train_from": "", 
    "train_steps": 100000, 
    "transformer_ff": 2048, 
    "truncated_decoder": 0, 
    "valid_batch_size": 32, 
    "valid_steps": 10000, 
    "warmup_steps": 4000, 
    "window_size": 0.02, 
    "word_vec_size": -1
}