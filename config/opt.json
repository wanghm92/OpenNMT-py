{
    "accum_count": 1, 
    "adagrad_accumulator_init": 0, 
    "adam_beta1": 0.9, 
    "adam_beta2": 0.999, 
    "batch_size": 64, 
    "batch_type": "sents", 
    "bridge": false, 
    "brnn": false, 
    "cnn_kernel_width": 3, 
    "context_gate": null, 
    "copy_attn": false, 
    "copy_attn_force": false, 
    "copy_loss_by_seqlength": false, 
    "coverage_attn": false, 
    "data": "data/recom_nlg/recom_nlg",
    "dec_layers": 1,
    "decay_method": "", 
    "decay_steps": 10000, 
    "decoder_type": "rnn", 
    "device_id": 0, 
    "dropout": 0.3, 
    "enc_layers": 1,
    "encoder_type": "brnn",
    "epochs": 0, 
    "exp": "", 
    "exp_host": "", 
    "feat_merge": "concat", 
    "feat_vec_exponent": 0.4,
    "feat_vec_size": 64,
    "fix_word_vecs_dec": false, 
    "fix_word_vecs_enc": false, 
    "global_attention": "general", 
    "gpu_backend": "nccl", 
    "gpu_rank": 0, 
    "gpu_verbose_level": 0,
    "gpuid": [
        0
    ], 
    "heads": 8, 
    "input_feed": 1, 
    "keep_checkpoint": -1, 
    "label_smoothing": 0.1,
    "lambda_coverage": 1, 
    "layers": -1, 
    "learning_rate": 0.001,
    "learning_rate_decay": 0.7,
    "log_file": "", 
    "max_generator_batches": 32, 
    "max_grad_norm": 15,
    "model_type": "text", 
    "normalization": "sents", 
    "optim": "adam",
    "param_init": 0.1, 
    "param_init_glorot": false, 
    "position_encoding": false, 
    "pre_word_vecs_dec": "data/recom_nlg/recom_nlg.emb.dec.pt",
    "pre_word_vecs_enc": "data/recom_nlg/recom_nlg.emb.enc.pt",
    "report_every": 500,
    "reuse_copy_attn": false, 
    "rnn_size": 500, 
    "rnn_type": "LSTM", 
    "sample_rate": 16000, 
    "save_checkpoint_steps": 1500,
    "save_model": "../onmt-py-output/temp",
    "seed": -1, 
    "self_attn_type": "scaled-dot", 
    "share_decoder_embeddings": false,
    "share_embeddings": true,
    "src_word_vec_size": 200,
    "start_decay_steps": 30000,
    "tensorboard": true,
    "tensorboard_log_dir": "runs/recom_nlg",
    "tgt_word_vec_size": 200,
    "train_from": "",
    "train_steps": 100000,
    "transformer_ff": 2048,
    "truncated_decoder": 0, 
    "valid_batch_size": 32, 
    "valid_steps": 2000,
    "warmup_steps": 4000, 
    "window_size": 0.02, 
    "word_vec_size": -1
}